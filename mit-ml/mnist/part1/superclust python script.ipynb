{"cells":[{"cell_type":"code","source":["def compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n    \"\"\"\n    Computes the total cost over every datapoint.\n\n    Args:\n        X - (n, d) NumPy array (n datapoints each with d features)\n        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n            data point\n        theta - (k, d) NumPy array, where row j represents the parameters of our\n                model for label j\n        lambda_factor - the regularization constant (scalar)\n        temp_parameter - the temperature parameter of softmax function (scalar)\n\n    Returns\n        c - the cost value (scalar)\n    \"\"\"\n    import gc\n    print(\"fdasfdas\")\n    stheta=sparse.coo_matrix(theta)\n    # YOUR CODE HERE\n    ex = np.exp(np.dot(theta, X.T)/temp_parameter)\n    ex2 = ex.sum(0)\n    tmpx=ex/ex2\n    p=tmpx > 0.0\n    res=np.zeros_like(tmpx)\n    res[p]=np.log(tmpx[p])\n    print(\"test1\")\n\n    # log = np.ma.log(ex/ex2)\n    # prepare equality calculation\n    tl = np.tile(np.arange(theta.shape[0]), (Y.shape[0], 1))\n    ts2 = np.tile(Y, (theta.shape[0], 1))\n    iseq = np.equal(ts2.T, tl)*1\n    print(\"test2\")\n    # final e\n    # part1 =  -1/Y.shape[0]*np.dot(iseq, log)  if log.ndim==0 else 0\n    del ex,ex2,theta,tl,ts2,tmpx,X\n    gc.collect()\n    print(\"memory clean\")\n    part1 =  -1/Y.shape[0]*np.dot(iseq, res)\n    print(\"test3\")\n    part2 = stheta.power(2).sum()*lambda_factor/2\n    tmp1 = part1  + part2\n    print(\"test4\")\n    res=tmp1.sum(1)[0]\n    \n    if np.isnan(res):\n        res=part2\n        \n    # if(tmp1.ndim==0)\n    # res = tmp1.sum(1) if log.ndim!=0 else part2 \n    return res"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["def run_softmax_on_MNIST(temp_parameter=1):\n    \"\"\"\n    Trains softmax, classifies test data, computes test error, and plots cost function\n\n    Runs softmax_regression on the MNIST training set and computes the test error using\n    the test set. It uses the following values for parameters:\n    alpha = 0.3\n    lambda = 1e-4\n    num_iterations = 150\n\n    Saves the final theta to ./theta.pkl.gz\n\n    Returns:\n        Final test error\n    \"\"\"\n    train_x, train_y, test_x, test_y = get_MNIST_data()\n    theta, cost_function_history = softmax_regression(train_x, train_y, temp_parameter, alpha=0.3, lambda_factor=1.0e-4, k=10, num_iterations=150)\n    plot_cost_function_over_time(cost_function_history)\n    test_error = compute_test_error(test_x, test_y, theta, temp_parameter)\n    # Save the model parameters theta obtained from calling softmax_regression to disk.\n    write_pickle_data(theta, \"./theta.pkl.gz\")\n\n    # TODO: add your code here for the \"Using the Current Model\" question in tab 4.\n    #      and print the test_error_mod3\n    return test_error"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["\ndef softmax_regression(X, Y, temp_parameter, alpha, lambda_factor, k, num_iterations):\n    \"\"\"\n    Runs batch gradient descent for a specified number of iterations on a dataset\n    with theta initialized to the all-zeros array. Here, theta is a k by d NumPy array\n    where row j represents the parameters of our model for label j for\n    j = 0, 1, ..., k-1\n\n    Args:\n        X - (n, d - 1) NumPy array (n data points, each with d-1 features)\n        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n            data point\n        temp_parameter - the temperature parameter of softmax function (scalar)\n        alpha - the learning rate (scalar)\n        lambda_factor - the regularization constant (scalar)\n        k - the number of labels (scalar)\n        num_iterations - the number of iterations to run gradient descent (scalar)\n\n    Returns:\n        theta - (k, d) NumPy array that is the final value of parameters theta\n        cost_function_progression - a Python list containing the cost calculated at each step of gradient descent\n    \"\"\"\n    X = augment_feature_vector(X)\n    theta = np.zeros([k, X.shape[1]])\n    cost_function_progression = []\n    for i in range(num_iterations):\n        cost_function_progression.append(compute_cost_function(\n            X, Y, theta, lambda_factor, temp_parameter))\n        theta = run_gradient_descent_iteration(\n            X, Y, theta, alpha, lambda_factor, temp_parameter)\n    return theta, cost_function_progression\n\n\ndef get_classification(X, theta, temp_parameter):\n    \"\"\"\n    Makes predictions by classifying a given dataset\n\n    Args:\n        X - (n, d - 1) NumPy array (n data points, each with d - 1 features)\n        theta - (k, d) NumPy array where row j represents the parameters of our model for\n                label j\n        temp_parameter - the temperature parameter of softmax function (scalar)\n\n    Returns:\n        Y - (n, ) NumPy array, containing the predicted label (a number between 0-9) for\n            each data point\n    \"\"\"\n    X = augment_feature_vector(X)\n    probabilities = compute_probabilities(X, theta, temp_parameter)\n    return np.argmax(probabilities, axis=0)\n\n\ndef plot_cost_function_over_time(cost_function_history):\n    plt.plot(range(len(cost_function_history)), cost_function_history)\n    plt.ylabel('Cost Function')\n    plt.xlabel('Iteration number')\n    plt.show()\n\n\ndef compute_test_error(X, Y, theta, temp_parameter):\n    error_count = 0.\n    assigned_labels = get_classification(X, theta, temp_parameter)\n    return 1 - np.mean(assigned_labels == Y)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["import pickle, gzip, numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport math\n\n\ndef plot_images(X):\n    if X.ndim == 1:\n        X = np.array([X])\n    num_images = X.shape[0]\n    num_rows = math.floor(math.sqrt(num_images))\n    num_cols = math.ceil(num_images/num_rows)\n    for i in range(num_images):\n        reshaped_image = X[i,:].reshape(28,28)\n        plt.subplot(num_rows, num_cols, i+1)\n        plt.imshow(reshaped_image, cmap = cm.Greys_r)\n        plt.axis('off')\n    plt.show()\n\n\ndef pick_examples_of(X, Y, labels, total_count):\n    bool_arr = None\n    for label in labels:\n        bool_arr_for_label = (Y == label)\n        if bool_arr is None:\n            bool_arr = bool_arr_for_label\n        else:\n            bool_arr |= bool_arr_for_label\n    filtered_x = X[bool_arr]\n    filtered_y = Y[bool_arr]\n    return (filtered_x[:total_count], filtered_y[:total_count])\n\n\ndef extract_training_and_test_examples_with_labels(train_x, train_y, test_x, test_y, labels, training_count, test_count):\n    filtered_train_x, filtered_train_y = pick_examples_of(train_x, train_y, labels, training_count)\n    filtered_test_x, filtered_test_y = pick_examples_of(test_x, test_y, labels, test_count)\n    return (filtered_train_x, filtered_train_y, filtered_test_x, filtered_test_y)\n\ndef write_pickle_data(data, file_name):\n    f = gzip.open(file_name, 'wb')\n    pickle.dump(data, f)\n    f.close()\n\ndef read_pickle_data(file_name):\n    f = gzip.open(file_name, 'rb')\n    data = pickle.load(f, encoding='latin1')\n    f.close()\n    return data\n\ndef get_MNIST_data():\n    \"\"\"\n    Reads mnist dataset from file\n\n    Returns:\n        train_x - 2D Numpy array (n, d) where each row is an image\n        train_y - 1D Numpy array (n, ) where each row is a label\n        test_x  - 2D Numpy array (n, d) where each row is an image\n        test_y  - 1D Numpy array (n, ) where each row is a label\n\n    \"\"\"\n    train_set, valid_set, test_set = read_pickle_data('/dbfs/FileStore/tables/mnist_pkl-d4040.gz') #file_location = \"/FileStore/tables/train_labels_mini_txt-d5d7d.gz\"\n    train_x, train_y = train_set\n    valid_x, valid_y = valid_set\n    train_x = np.vstack((train_x, valid_x))\n    train_y = np.append(train_y, valid_y)\n    test_x, test_y = test_set\n    return (train_x, train_y, test_x, test_y)\n\ndef load_train_and_test_pickle(file_name):\n    train_x, train_y, test_x, test_y = read_pickle_data(file_name)\n    return train_x, train_y, test_x, test_y\n\n# returns the feature set in a numpy ndarray\ndef load_CSV(filename):\n    stuff = np.asarray(np.loadtxt(open(filename, 'rb'), delimiter=','))\n    return stuff\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["def augment_feature_vector(X):\n    \"\"\"\n    Adds the x[i][0] = 1 feature for each data point x[i].\n\n    Args:\n        X - a NumPy matrix of n data points, each with d - 1 features\n\n    Returns: X_augment, an (n, d) NumPy array with the added feature for each datapoint\n    \"\"\"\n    column_of_ones = np.zeros([len(X), 1]) + 1\n    return np.hstack((column_of_ones, X))\n\n\ndef compute_probabilities(X, theta, temp_parameter):\n    \"\"\"\n    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n    for j = 0, 1, ..., k-1\n\n    Args:\n        X - (n, d) NumPy array (n datapoints each with d features)\n        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n        temp_parameter - the temperature parameter of softmax function (scalar)\n    Returns:\n        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n    \"\"\"\n    # YOUR CODE HERE\n    p1 = np.dot(theta, X.T)/temp_parameter\n    c = p1.max(0)\n    p1 = np.exp(np.dot(theta, X.T)/temp_parameter-c)\n    norm = 1/p1.sum(0)\n    return norm*p1\n\n\ndef compute_cost_function(X, Y, theta, lambda_factor, temp_parameter):\n    \"\"\"\n    Computes the total cost over every datapoint.\n\n    Args:\n        X - (n, d) NumPy array (n datapoints each with d features)\n        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n            data point\n        theta - (k, d) NumPy array, where row j represents the parameters of our\n                model for label j\n        lambda_factor - the regularization constant (scalar)\n        temp_parameter - the temperature parameter of softmax function (scalar)\n\n    Returns\n        c - the cost value (scalar)\n    \"\"\"\n    import gc\n    stheta=sparse.coo_matrix(theta)\n    # YOUR CODE HERE\n    ex = np.exp(np.dot(stheta, X.T)/temp_parameter)\n    ex2 = ex.sum(0)\n    tmpx=ex/ex2\n    p=tmpx > 0.0\n    res=np.zeros_like(tmpx)\n    res[p]=np.log(tmpx[p])\n\n    del theta,X,ex,ex2,tmpx\n    gc.collect()\n    # log = np.ma.log(ex/ex2)\n    # prepare equality calculation\n    tl = np.tile(np.arange(theta.shape[0]), (Y.shape[0], 1))\n    ts2 = np.tile(Y, (theta.shape[0], 1))\n    stl=sparse.coo_matrix(tl)\n    sts2=sparse.coo_matrix(ts2)\n    del tl,ts2\n    siseq = (sts2.T == stl)\n    del stl,sts2\n    gc.collect()\n    # final e\n    # part1 =  -1/Y.shape[0]*np.dot(iseq, log)  if log.ndim==0 else 0\n    part1 =  -1/Y.shape[0]*np.dot(siseq, res)\n    del siseq,res\n    gc.collect()\n    part2 = stheta.power(2).sum()*lambda_factor/2\n    del stheta\n    tmp1 = part1  + part2\n    del part1,part2\n    gc.collect()\n    tmp1s=sparse.coo_matrix(tmp1)\n    res=tmp1s.sum(1)[0]\n    \n    if np.isnan(res):\n        res=part2\n        \n    # if(tmp1.ndim==0)\n    # res = tmp1.sum(1) if log.ndim!=0 else part2 \n    return res\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["import scipy.sparse as sparse"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["def compute_probabilities(X, theta, temp_parameter):\n    \"\"\"\n    Computes, for each datapoint X[i], the probability that X[i] is labeled as j\n    for j = 0, 1, ..., k-1\n\n    Args:\n        X - (n, d) NumPy array (n datapoints each with d features)\n        theta - (k, d) NumPy array, where row j represents the parameters of our model for label j\n        temp_parameter - the temperature parameter of softmax function (scalar)\n    Returns:\n        H - (k, n) NumPy array, where each entry H[j][i] is the probability that X[i] is labeled as j\n    \"\"\"\n    # YOUR CODE HERE\n    p1 = np.dot(theta, X.T)/temp_parameter\n    c = p1.max(0)\n    p1 = np.exp(np.dot(theta, X.T)/temp_parameter-c)\n    norm = 1/p1.sum(0)\n    return norm*p1\n  \ndef run_gradient_descent_iteration(X, Y, theta, alpha, lambda_factor, temp_parameter):\n    \"\"\"\n    Runs one step of batch gradient descent\n\n    Args:\n        X - (n, d) NumPy array (n datapoints each with d features)\n        Y - (n, ) NumPy array containing the labels (a number from 0-9) for each\n            data point\n        theta - (k, d) NumPy array, where row j represents the parameters of our\n                model for label j\n        alpha - the learning rate (scalar)\n        lambda_factor - the regularization constant (scalar)\n        temp_parameter - the temperature parameter of softmax function (scalar)\n\n    Returns:\n        theta - (k, d) NumPy array that is the final value of parameters theta\n    \"\"\"\n    # YOUR CODE HERE\n    p1 = -1/(Y.shape[0]*temp_parameter)\n    ex = np.exp(np.dot(theta, X.T)/temp_parameter)\n    ex2 = ex.sum(0)\n\n    tmpx=ex/ex2\n    p=tmpx > 0.0\n    res=np.zeros_like(tmpx)\n    res[p]=tmpx[p]\n\n    tl = np.tile(np.arange(theta.shape[0]), (Y.shape[0], 1))\n    ts2 = np.tile(Y, (theta.shape[0], 1))\n    iseq = np.equal(ts2.T, tl)*1\n    H=compute_probabilities(X,theta,temp_parameter)\n    #(iseq-res.T)\n    #H => H.T\n    gradtheta = (p1*np.dot(X.T, (iseq-H.T))).T + lambda_factor*theta\n\n    return theta - alpha*gradtheta"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["print('softmax test_error=', run_softmax_on_MNIST(temp_parameter=1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">fdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nfdasfdas\ntest1\ntest2\nmemory clean\ntest3\ntest4\nsoftmax test_error= 0.1005\n</div>"]}}],"execution_count":8}],"metadata":{"name":"superclust python script","notebookId":906113023903558},"nbformat":4,"nbformat_minor":0}
